{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yash/Desktop/Python/Data_Sets/MNIST/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.7372549  1.         0.36862745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.74901961 0.98039216 0.99215686\n",
      "  0.36470588 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.48235294 0.97254902 0.99215686 0.65490196 0.03921569 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31372549 0.96862745 0.99215686\n",
      "  0.81568627 0.05098039 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.11372549 0.81176471 0.99215686 0.92156863 0.30196078 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21176471 0.81960784 0.99215686\n",
      "  0.99215686 0.34509804 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.36470588 0.99607843 0.99215686 0.93333333 0.66666667 0.06666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.09019608 0.82352941 0.99607843\n",
      "  0.99215686 0.62352941 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0627451  0.81960784 0.99215686 0.99607843 0.94117647 0.31764706\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.10588235 0.99215686\n",
      "  0.99215686 0.99607843 0.05098039 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07843137 0.80784314 0.99607843 0.99607843 0.77647059\n",
      "  0.02745098 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.65882353\n",
      "  0.99215686 0.99215686 0.76862745 0.02745098 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.07843137 0.79607843 0.99215686 0.97254902\n",
      "  0.29803922 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.08627451\n",
      "  0.7372549  0.99215686 0.96078431 0.36470588 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.40392157 0.99215686 0.99215686\n",
      "  0.74901961 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34901961 0.94117647 0.99215686 0.76470588 0.09803922 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05882353 0.8627451  0.99215686\n",
      "  0.99215686 0.31372549 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36862745 0.99215686 0.99215686 0.99215686 0.36862745\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.34901961\n",
      "  0.98431373 0.99215686 0.98039216 0.51372549 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.83921569 0.85490196\n",
      "  0.37254902 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "(1, 784)\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_ = np.array(df)\n",
    "X = X_[:,1:]\n",
    "a = X[:1,:]\n",
    "X = X/255.0\n",
    "a_g = X[:1,:]\n",
    "y = X_[:,0]\n",
    "\n",
    "#print (a)\n",
    "print (a.shape)\n",
    "\n",
    "print (a_g)\n",
    "print (a_g.shape)\n",
    "a = a.reshape(-1)\n",
    "a_g = a_g.reshape(-1)\n",
    "\n",
    "print (a.shape)\n",
    "a = a.reshape((28,28))\n",
    "a_g = a_g.reshape((28,28))\n",
    "print (a.shape)\n",
    "#plt.imshow(a)\n",
    "#plt.imshow(a_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (42000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:8000,:]\n",
    "X_val = X[8000:,:]\n",
    "y_train = y[:8000]\n",
    "y_val = y[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28*28\n",
    "H1_SIZE = 256\n",
    "H2_SIZE = 64\n",
    "OUT_SIZE = 10\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 50\n",
    "ALPHA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,y):\n",
    "    return (100.0* np.sum(pred==y) / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weights():\n",
    "    np.random.seed(0)\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(IMG_SIZE,H1_SIZE)/ np.sqrt(IMG_SIZE)\n",
    "    model['B1'] = np.zeros((1,H1_SIZE))\n",
    "    model['W2'] = np.random.randn(H1_SIZE,H2_SIZE)/ np.sqrt(H1_SIZE)\n",
    "    model['B2'] = np.zeros((1,H2_SIZE))\n",
    "    model['W3'] = np.random.randn(H2_SIZE,OUT_SIZE)/ np.sqrt(H2_SIZE)\n",
    "    model['B3'] = np.zeros((1,OUT_SIZE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(model,x):\n",
    "    z1 = x.dot(model['W1']) + model['B1']\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(model['W2']) + model['B2']\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(model['W3']) + model['B3']\n",
    "    h_x = np.exp(z3)\n",
    "    y_out = h_x/ np.sum(h_x, axis=1, keepdims=True)\n",
    "    return a1, a2, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(model, x ,a1 , a2, y, y_out):\n",
    "    delta4 = y_out\n",
    "    delta4[range(y.shape[0]), y] -= 1\n",
    "    dw3 = (a2.T).dot(delta4)\n",
    "    db3 = np.sum(delta4, axis = 0)\n",
    "    delta3 = (1 - np.square(a2))*delta4.dot(model['W3'].T)\n",
    "    dw2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis = 0)\n",
    "    delta2 = (1 - np.square(a1))*delta3.dot(model['W2'].T)\n",
    "    dw1 = (x.T).dot(delta2)\n",
    "    db1 = np.sum(delta2, axis = 0)\n",
    "    \n",
    "    model['W1'] += -ALPHA*dw1\n",
    "    model['B1'] += -ALPHA*db1\n",
    "    model['W2'] += -ALPHA*dw2\n",
    "    model['B2'] += -ALPHA*db2\n",
    "    model['W3'] += -ALPHA*dw3\n",
    "    model['B3'] += -ALPHA*db3\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, p, y):\n",
    "    correct_logprobs = -np.log(p[range(y.shape[0]),y])\n",
    "    l = np.sum(correct_logprobs)\n",
    "    \n",
    "    return 1.0/y.shape[0] * l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(y_out):\n",
    "    return np.argmax(y_out, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = initial_weights()\n",
    "    for ix in range(EPOCH):\n",
    "        print (\"\\nEpoch : %d\" %(ix+1))\n",
    "        count = 0\n",
    "        while (count+BATCH_SIZE) < y_train.shape[0]:\n",
    "            batch_data = X_train[count:(count+BATCH_SIZE),:]\n",
    "            batch_labels = y_train[count:(count+BATCH_SIZE),]\n",
    "            count += BATCH_SIZE\n",
    "            \n",
    "            a1, a2 , p = forward_prop(model, batch_data)\n",
    "            model = back_prop(model,batch_data,a1,a2,batch_labels,p)\n",
    "        \n",
    "        _,_, p = forward_prop(model, X_train)\n",
    "        print ('training_loss : % .3f' % (loss(model,p,y_train)))\n",
    "        _,_,p = forward_prop(model, X_val)\n",
    "        pred = predict(p)\n",
    "        print ('val_accuracy : % .3f' % (accuracy(pred,y_val)))\n",
    "        print ('val_loss : % .3f' % loss(model,p,y_val))\n",
    "    print(\"*************Completed***********\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 1\n",
      "training_loss :  0.545\n",
      "val_accuracy :  86.162\n",
      "val_loss :  0.563\n",
      "\n",
      "Epoch : 2\n",
      "training_loss :  0.397\n",
      "val_accuracy :  88.329\n",
      "val_loss :  0.420\n",
      "\n",
      "Epoch : 3\n",
      "training_loss :  0.338\n",
      "val_accuracy :  89.353\n",
      "val_loss :  0.369\n",
      "\n",
      "Epoch : 4\n",
      "training_loss :  0.302\n",
      "val_accuracy :  89.985\n",
      "val_loss :  0.341\n",
      "\n",
      "Epoch : 5\n",
      "training_loss :  0.276\n",
      "val_accuracy :  90.482\n",
      "val_loss :  0.322\n",
      "\n",
      "Epoch : 6\n",
      "training_loss :  0.255\n",
      "val_accuracy :  90.879\n",
      "val_loss :  0.307\n",
      "\n",
      "Epoch : 7\n",
      "training_loss :  0.237\n",
      "val_accuracy :  91.147\n",
      "val_loss :  0.296\n",
      "\n",
      "Epoch : 8\n",
      "training_loss :  0.221\n",
      "val_accuracy :  91.426\n",
      "val_loss :  0.286\n",
      "\n",
      "Epoch : 9\n",
      "training_loss :  0.207\n",
      "val_accuracy :  91.706\n",
      "val_loss :  0.277\n",
      "\n",
      "Epoch : 10\n",
      "training_loss :  0.194\n",
      "val_accuracy :  91.879\n",
      "val_loss :  0.270\n",
      "\n",
      "Epoch : 11\n",
      "training_loss :  0.183\n",
      "val_accuracy :  92.088\n",
      "val_loss :  0.264\n",
      "\n",
      "Epoch : 12\n",
      "training_loss :  0.173\n",
      "val_accuracy :  92.285\n",
      "val_loss :  0.258\n",
      "\n",
      "Epoch : 13\n",
      "training_loss :  0.164\n",
      "val_accuracy :  92.444\n",
      "val_loss :  0.253\n",
      "\n",
      "Epoch : 14\n",
      "training_loss :  0.156\n",
      "val_accuracy :  92.600\n",
      "val_loss :  0.249\n",
      "\n",
      "Epoch : 15\n",
      "training_loss :  0.148\n",
      "val_accuracy :  92.694\n",
      "val_loss :  0.245\n",
      "\n",
      "Epoch : 16\n",
      "training_loss :  0.141\n",
      "val_accuracy :  92.838\n",
      "val_loss :  0.242\n",
      "\n",
      "Epoch : 17\n",
      "training_loss :  0.134\n",
      "val_accuracy :  92.900\n",
      "val_loss :  0.239\n",
      "\n",
      "Epoch : 18\n",
      "training_loss :  0.128\n",
      "val_accuracy :  93.000\n",
      "val_loss :  0.236\n",
      "\n",
      "Epoch : 19\n",
      "training_loss :  0.122\n",
      "val_accuracy :  93.071\n",
      "val_loss :  0.234\n",
      "\n",
      "Epoch : 20\n",
      "training_loss :  0.117\n",
      "val_accuracy :  93.141\n",
      "val_loss :  0.232\n",
      "\n",
      "Epoch : 21\n",
      "training_loss :  0.111\n",
      "val_accuracy :  93.188\n",
      "val_loss :  0.229\n",
      "\n",
      "Epoch : 22\n",
      "training_loss :  0.106\n",
      "val_accuracy :  93.259\n",
      "val_loss :  0.228\n",
      "\n",
      "Epoch : 23\n",
      "training_loss :  0.102\n",
      "val_accuracy :  93.371\n",
      "val_loss :  0.226\n",
      "\n",
      "Epoch : 24\n",
      "training_loss :  0.097\n",
      "val_accuracy :  93.429\n",
      "val_loss :  0.224\n",
      "\n",
      "Epoch : 25\n",
      "training_loss :  0.092\n",
      "val_accuracy :  93.497\n",
      "val_loss :  0.223\n",
      "\n",
      "Epoch : 26\n",
      "training_loss :  0.088\n",
      "val_accuracy :  93.524\n",
      "val_loss :  0.221\n",
      "\n",
      "Epoch : 27\n",
      "training_loss :  0.084\n",
      "val_accuracy :  93.574\n",
      "val_loss :  0.220\n",
      "\n",
      "Epoch : 28\n",
      "training_loss :  0.080\n",
      "val_accuracy :  93.629\n",
      "val_loss :  0.219\n",
      "\n",
      "Epoch : 29\n",
      "training_loss :  0.076\n",
      "val_accuracy :  93.685\n",
      "val_loss :  0.217\n",
      "\n",
      "Epoch : 30\n",
      "training_loss :  0.073\n",
      "val_accuracy :  93.747\n",
      "val_loss :  0.216\n",
      "\n",
      "Epoch : 31\n",
      "training_loss :  0.069\n",
      "val_accuracy :  93.800\n",
      "val_loss :  0.215\n",
      "\n",
      "Epoch : 32\n",
      "training_loss :  0.066\n",
      "val_accuracy :  93.806\n",
      "val_loss :  0.214\n",
      "\n",
      "Epoch : 33\n",
      "training_loss :  0.063\n",
      "val_accuracy :  93.844\n",
      "val_loss :  0.214\n",
      "\n",
      "Epoch : 34\n",
      "training_loss :  0.060\n",
      "val_accuracy :  93.868\n",
      "val_loss :  0.213\n",
      "\n",
      "Epoch : 35\n",
      "training_loss :  0.057\n",
      "val_accuracy :  93.891\n",
      "val_loss :  0.212\n",
      "\n",
      "Epoch : 36\n",
      "training_loss :  0.055\n",
      "val_accuracy :  93.915\n",
      "val_loss :  0.212\n",
      "\n",
      "Epoch : 37\n",
      "training_loss :  0.052\n",
      "val_accuracy :  93.932\n",
      "val_loss :  0.211\n",
      "\n",
      "Epoch : 38\n",
      "training_loss :  0.050\n",
      "val_accuracy :  93.938\n",
      "val_loss :  0.211\n",
      "\n",
      "Epoch : 39\n",
      "training_loss :  0.048\n",
      "val_accuracy :  93.959\n",
      "val_loss :  0.210\n",
      "\n",
      "Epoch : 40\n",
      "training_loss :  0.045\n",
      "val_accuracy :  93.988\n",
      "val_loss :  0.210\n",
      "\n",
      "Epoch : 41\n",
      "training_loss :  0.043\n",
      "val_accuracy :  94.015\n",
      "val_loss :  0.210\n",
      "\n",
      "Epoch : 42\n",
      "training_loss :  0.041\n",
      "val_accuracy :  94.009\n",
      "val_loss :  0.209\n",
      "\n",
      "Epoch : 43\n",
      "training_loss :  0.040\n",
      "val_accuracy :  94.029\n",
      "val_loss :  0.209\n",
      "\n",
      "Epoch : 44\n",
      "training_loss :  0.038\n",
      "val_accuracy :  94.047\n",
      "val_loss :  0.209\n",
      "\n",
      "Epoch : 45\n",
      "training_loss :  0.036\n",
      "val_accuracy :  94.068\n",
      "val_loss :  0.209\n",
      "\n",
      "Epoch : 46\n",
      "training_loss :  0.035\n",
      "val_accuracy :  94.088\n",
      "val_loss :  0.209\n",
      "\n",
      "Epoch : 47\n",
      "training_loss :  0.033\n",
      "val_accuracy :  94.103\n",
      "val_loss :  0.209\n",
      "\n",
      "Epoch : 48\n",
      "training_loss :  0.032\n",
      "val_accuracy :  94.115\n",
      "val_loss :  0.208\n",
      "\n",
      "Epoch : 49\n",
      "training_loss :  0.030\n",
      "val_accuracy :  94.129\n",
      "val_loss :  0.208\n",
      "\n",
      "Epoch : 50\n",
      "training_loss :  0.029\n",
      "val_accuracy :  94.150\n",
      "val_loss :  0.208\n",
      "*************Completed***********\n"
     ]
    }
   ],
   "source": [
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/yash/Desktop/Python/Data_Sets/MNIST/test.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X[TEST_NUM:TEST_NUM+1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "a = a.reshape(-1,)\n",
    "print (a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "a = a.reshape((28,28))\n",
    "print (a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f036c1e2748>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(a,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_, y_out = forward_prop(model,X)\n",
    "one_hot_ans = y_out[TEST_NUM:TEST_NUM+1,:]\n",
    "one_hot_ans = np.array(one_hot_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_ans.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
